{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLMmxAazaKKh",
        "outputId": "88556676-0272-4693-e90e-14c409b94a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmpdulEAaIEH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import networkx as nx\n",
        "import torch_geometric as pyg\n",
        "from torch_geometric.data import Data\n",
        "import torch_geometric as pyg\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_add_pool\n",
        "import math\n",
        "import torch.sparse\n",
        "from torch.nn import Parameter\n",
        "from torch.nn import Module\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import random\n",
        "import json\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAEZ6iE0ZSXl",
        "outputId": "7fcaf776-7297-48b8-c2a0-bf7bb1a41288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBDvzDEzc0BX"
      },
      "outputs": [],
      "source": [
        "root = '/content/drive/My Drive/PPRGNN-Dataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM_zk_Ban8cQ"
      },
      "source": [
        "# Load Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcbgTm4xIdFm"
      },
      "outputs": [],
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx, device=None):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
        "    if device is not None:\n",
        "        tensor = tensor.to(device)\n",
        "    return tensor\n",
        "\n",
        "def load_raw_graph(dataset_str = \"amazon-all\", self_loops=True, root='dataset/'):\n",
        "    txt_file = root + dataset_str + '/adj_list.txt'\n",
        "    graph = {}\n",
        "    with open(txt_file, 'r') as f:\n",
        "        cur_idx = 0\n",
        "        for row in f:\n",
        "            row = row.strip().split()\n",
        "            adjs = []\n",
        "            for j in range(1, len(row)):\n",
        "                adjs.append(int(row[j]))\n",
        "            graph[cur_idx] = adjs\n",
        "            cur_idx += 1\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
        "    G = Data(edge_index=adj.coalesce().indices())\n",
        "    G = pyg.transforms.GCNNorm(add_self_loops=self_loops)(G)\n",
        "    adj = torch.sparse.FloatTensor(G.edge_index, G.edge_weight, torch.Size(adj.shape))\n",
        "\n",
        "    return adj\n",
        "\n",
        "'''\n",
        "def load_txt_data_v1(dataset_str=\"amazon-all\", portion='0.06', self_loops=True, root='dataset/'):\n",
        "    adj = load_raw_graph(dataset_str, self_loops, root)\n",
        "    labels = np.loadtxt(root + dataset_str + '/label.txt')\n",
        "    # Approximate multi-label labels as single-label\n",
        "    dominant_labels = torch.argmax(torch.FloatTensor(labels), dim=1).numpy()\n",
        "\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, train_size=float(portion), test_size=0.1, random_state=42)\n",
        "    for train_idx, val_idx in sss.split(np.zeros(len(dominant_labels)), dominant_labels):\n",
        "        idx_train = torch.LongTensor(train_idx)\n",
        "        idx_val = torch.LongTensor(val_idx)\n",
        "    # idx_train = list(np.loadtxt(root + dataset_str + '/train_idx-' + str(portion) + '.txt', dtype=int))\n",
        "    # idx_val = list(np.loadtxt(root + dataset_str + '/test_idx.txt', dtype=int))\n",
        "\n",
        "    idx_test = list(np.loadtxt(root + dataset_str + '/test_idx.txt', dtype=int))\n",
        "\n",
        "    with open(root + dataset_str + '/meta.txt', 'r') as f:\n",
        "        num_nodes, num_class = [int(w) for w in f.readline().strip().split()]\n",
        "\n",
        "    features = sp.identity(num_nodes)\n",
        "\n",
        "    # porting to pytorch\n",
        "    features = sparse_mx_to_torch_sparse_tensor(features).float()\n",
        "    labels = torch.FloatTensor(labels)\n",
        "    # labels = torch.max(labels, dim=1)[1]\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test, num_nodes, num_class\n",
        "'''\n",
        "\n",
        "'''\n",
        "def load_txt_data_v1(dataset_str=\"amazon-all\", portion='0.06', self_loops=True, root='dataset/'):\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import scipy.sparse as sp\n",
        "    from collections import defaultdict\n",
        "\n",
        "    adj = load_raw_graph(dataset_str, self_loops, root)\n",
        "\n",
        "    # Load all labels and convert to single-label (dominant class)\n",
        "    labels = np.loadtxt(root + dataset_str + '/label.txt')\n",
        "    dominant_labels = torch.argmax(torch.FloatTensor(labels), dim=1).numpy()\n",
        "    total_nodes = len(dominant_labels)\n",
        "\n",
        "    # Determine total number of training samples based on portion\n",
        "    portion = float(portion)\n",
        "    total_target = int(total_nodes * portion)\n",
        "\n",
        "    # Build class-to-index mapping\n",
        "    class_indices = defaultdict(list)\n",
        "    for idx, label in enumerate(dominant_labels):\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "    num_classes = len(class_indices)\n",
        "    available_classes = [cls for cls in class_indices if len(class_indices[cls]) > 0]\n",
        "    num_available_classes = len(available_classes)\n",
        "\n",
        "    # Compute uniform sample size per class (subject to availability)\n",
        "    per_class_quota = total_target // num_available_classes\n",
        "\n",
        "    selected_train_indices = []\n",
        "    for label in sorted(available_classes):\n",
        "        indices = class_indices[label]\n",
        "        if len(indices) >= per_class_quota:\n",
        "            chosen = np.random.choice(indices, per_class_quota, replace=False)\n",
        "        else:\n",
        "            chosen = indices  # Use all available if fewer than quota\n",
        "        selected_train_indices.extend(chosen)\n",
        "\n",
        "    # If too few total samples due to small classes, fill up randomly\n",
        "    if len(selected_train_indices) < total_target:\n",
        "        remaining = total_target - len(selected_train_indices)\n",
        "        all_remaining_indices = list(set(range(total_nodes)) - set(selected_train_indices))\n",
        "        np.random.shuffle(all_remaining_indices)\n",
        "        selected_train_indices.extend(all_remaining_indices[:remaining])\n",
        "\n",
        "    idx_train = torch.LongTensor(np.array(selected_train_indices))\n",
        "    idx_val = torch.LongTensor(np.loadtxt(root + dataset_str + '/test_idx.txt', dtype=int))\n",
        "    idx_test = torch.LongTensor(np.loadtxt(root + dataset_str + '/test_idx.txt', dtype=int))\n",
        "\n",
        "    with open(root + dataset_str + '/meta.txt', 'r') as f:\n",
        "        num_nodes, num_class = [int(w) for w in f.readline().strip().split()]\n",
        "\n",
        "    # Use identity matrix as features\n",
        "    features = sp.identity(num_nodes)\n",
        "    features = sparse_mx_to_torch_sparse_tensor(features).float()\n",
        "    labels = torch.FloatTensor(labels)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test, num_nodes, num_class\n",
        "'''\n",
        "\n",
        "\n",
        "def load_txt_data(dataset_str=\"amazon-all\", portion = '0.06' , self_loops=True, root='dataset/'):\n",
        "    adj = load_raw_graph(dataset_str, self_loops, root)\n",
        "    idx_train = list(np.loadtxt(root + dataset_str + '/train_idx-' + str(portion) + '.txt', dtype=int))\n",
        "    idx_val = list(np.loadtxt(root + dataset_str + '/test_idx.txt', dtype=int))\n",
        "    idx_test = list(np.loadtxt(root + dataset_str + '/test_idx.txt', dtype=int))\n",
        "    labels = np.loadtxt(root + dataset_str + '/label.txt')\n",
        "\n",
        "    with open(root + dataset_str + '/meta.txt', 'r') as f:\n",
        "        num_nodes, num_class = [int(w) for w in f.readline().strip().split()]\n",
        "\n",
        "    features = sp.identity(num_nodes)\n",
        "    # print(idx_train)\n",
        "    # porting to pytorch\n",
        "    features = sparse_mx_to_torch_sparse_tensor(features).float()\n",
        "    labels = torch.FloatTensor(labels)\n",
        "    print(labels.sum(dim=1))  # If sum > 1 for some rows, it's multi-label\n",
        "    #‌print(labels[0])\n",
        "    # print(labels)\n",
        "    # labels = torch.max(labels, dim=1)[1]\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test, num_nodes, num_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrTxwHjJpLQh"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQOzS8LVpmjg"
      },
      "outputs": [],
      "source": [
        "class PPRFunction(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, W, B, A, X_0, eps=1):\n",
        "        X_0 = B if X_0 is None else X_0\n",
        "        At = torch.transpose(A, 0, 1)\n",
        "        epsilon = eps\n",
        "        offset = 5\n",
        "        X_new, X_prev, i, converge_index = PPRFunction.evaluate(B, W, At, X_0, 0, epsilon, offset)\n",
        "        ctx.save_for_backward(W, B, A, X_prev)\n",
        "        ctx.max_index = i\n",
        "        ctx.epsilon = epsilon\n",
        "        ctx.offset = offset\n",
        "        return X_new, converge_index\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_x_i(B, W, At, X, i, stop_index, epsilon):\n",
        "        for j in range(i, stop_index - 1, -1):\n",
        "            X = torch.matmul(W / (1 + (j * epsilon)), X)\n",
        "            X = torch.spmm(At, X.T).T\n",
        "            X += B\n",
        "            X = torch.nn.functional.relu(X)\n",
        "        return X\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate(B, W, At, X_0, stop_index=0, epsilon=10, offset=5):\n",
        "        k = 300\n",
        "        h_stop_index = stop_index + offset\n",
        "        X_ = B\n",
        "        for converge_index in range(k):\n",
        "            X_ = torch.matmul(W / (1 + (converge_index * epsilon)), X_)\n",
        "            X_ = torch.spmm(At, X_.T).T\n",
        "            X_ = torch.nn.functional.relu(X_)\n",
        "            if torch.norm(X_, np.inf) < 3e-6:\n",
        "                break\n",
        "        X_prev = PPRFunction.evaluate_x_i(B, W, At, X_0, converge_index + offset, h_stop_index, epsilon)\n",
        "        X_new = PPRFunction.evaluate_x_i(B, W, At, X_prev, offset - 1, stop_index, epsilon)\n",
        "        return X_new, X_prev, converge_index + offset, converge_index\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output, _):\n",
        "        W, B, A, X = ctx.saved_tensors\n",
        "        At = torch.transpose(A, 0, 1)\n",
        "        epsilon = ctx.epsilon\n",
        "        offset = ctx.offset\n",
        "        dL_dQ = grad_output\n",
        "        dL_dW = torch.zeros_like(W)\n",
        "        dL_dB = torch.zeros_like(B)\n",
        "        finished = False\n",
        "        old_offset = 0\n",
        "\n",
        "        forward_W = W\n",
        "        backward_W = W.T\n",
        "        while not finished:\n",
        "            for i in range(old_offset, offset):\n",
        "                X_i = PPRFunction.evaluate_x_i(B, forward_W, At, X, i=offset - 1, stop_index=i + 1, epsilon=epsilon)\n",
        "                Z_i = torch.spmm(At, torch.matmul(forward_W / (1 + i*epsilon), X_i).T).T + B\n",
        "                dL_dQ_new = (Z_i > 0) * dL_dQ\n",
        "                dL_dB += dL_dQ_new\n",
        "                dL_dQ_new = torch.spmm(A / (1 + i*epsilon), dL_dQ_new.T).T\n",
        "                dL_dW += torch.matmul(dL_dQ_new, X_i.T)\n",
        "                dL_dQ_new = torch.matmul(backward_W, dL_dQ_new)\n",
        "                err_q = torch.norm(dL_dQ - dL_dQ_new, np.inf)\n",
        "                dL_dQ = dL_dQ_new\n",
        "                if err_q < 1e-5 or i == 5:\n",
        "                    finished = True\n",
        "                    break\n",
        "            if finished:\n",
        "                break\n",
        "            else:\n",
        "                X = PPRFunction.evaluate_x_i(B, W, At, X, i=ctx.max_index + offset, stop_index=offset, epsilon=epsilon)\n",
        "                old_offset = offset\n",
        "                offset = offset + ctx.offset\n",
        "        return dL_dW, dL_dB, None, None, None, None, None, None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9Ur-Npjpc6q"
      },
      "outputs": [],
      "source": [
        "class PPRLayer(Module):\n",
        "    def __init__(self, in_features, out_features, eps, **kwargs):\n",
        "        super(PPRLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.eps = eps\n",
        "\n",
        "        self.W = Parameter(torch.FloatTensor(self.out_features, self.out_features))\n",
        "        self.Omega_1 = Parameter(torch.FloatTensor(self.out_features, self.in_features))\n",
        "        self.init()\n",
        "\n",
        "    def init(self):\n",
        "        stdv = 1. / math.sqrt(self.W.size(1))\n",
        "        self.W.data.uniform_(-stdv, stdv)\n",
        "        self.Omega_1.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, A, X_0, *args):\n",
        "        x = torch.spmm(torch.transpose(x, 0, 1), self.Omega_1.T).T\n",
        "        x = torch.spmm(torch.transpose(A, 0, 1), x.T).T\n",
        "        x = PPRFunction.apply(self.W, x, A, X_0, self.eps)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDZglUBBpHGJ"
      },
      "outputs": [],
      "source": [
        "class PPRGNN_Amazon(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, out_dim, dropout, num_nodes=None, **kwargs):\n",
        "        super(PPRGNN_Amazon, self).__init__()\n",
        "\n",
        "        self.conv = PPRLayer(in_dim, h_dim, **kwargs)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.X_0 = nn.Parameter(torch.zeros(h_dim, num_nodes), requires_grad=False)\n",
        "        self.V = nn.Linear(h_dim, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, x, edge_index=None, edge_weight=None, adj=None):\n",
        "        if adj is None:\n",
        "            adj = torch.sparse.FloatTensor(edge_index, edge_weight, torch.Size([x.shape[0], x.shape[0]]))\n",
        "            x = x.T\n",
        "\n",
        "        x, converge_index = self.conv(x, adj, self.X_0)\n",
        "        x = F.dropout(x.T, self.dropout, training=self.training)\n",
        "        x = self.V(x)\n",
        "        return x, converge_index\n",
        "\n",
        "\n",
        "class PPRGNN_PPI(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, h_dim, **kwargs):\n",
        "        super(PPRGNN_PPI, self).__init__()\n",
        "        self.conv1 = PPRLayer(in_dim, 4 * h_dim, **kwargs)\n",
        "        self.conv2 = PPRLayer(4 * h_dim, 2 * h_dim, **kwargs)\n",
        "        self.conv3 = PPRLayer(2 * h_dim, 2 * h_dim, **kwargs)\n",
        "        self.conv4 = PPRLayer(2 * h_dim, h_dim, **kwargs)\n",
        "        self.conv5 = PPRLayer(h_dim, out_dim, **kwargs)\n",
        "\n",
        "        self.X_0 = None\n",
        "\n",
        "        self.V = nn.Linear(h_dim, out_dim)\n",
        "        self.V_0 = nn.Linear(in_dim, 4 * h_dim)\n",
        "        self.V_1 = nn.Linear(4 * h_dim, 2 * h_dim)\n",
        "        self.V_2 = nn.Linear(2 * h_dim, 2 * h_dim)\n",
        "        self.V_3 = nn.Linear(2 * h_dim, h_dim)\n",
        "\n",
        "    def forward(self, features, edge_index, edge_weight):\n",
        "        features = features.T\n",
        "        adj = torch.sparse.FloatTensor(edge_index, edge_weight, torch.Size([features.shape[1], features.shape[1]]))\n",
        "\n",
        "        x = features\n",
        "        x_new, layers1 = self.conv1(x, adj, self.X_0)\n",
        "        x = F.elu(x_new.T + self.V_0(x.T)).T\n",
        "        x_new, layers2 = self.conv2(x, adj, self.X_0)\n",
        "        x = F.elu(x_new.T + self.V_1(x.T)).T\n",
        "        x_new, layers3 = self.conv3(x, adj, self.X_0)\n",
        "        x = F.elu(x_new.T + self.V_2(x.T)).T\n",
        "        x_new, layers4 = self.conv4(x, adj, self.X_0)\n",
        "        x = F.elu(x_new.T + self.V_3(x.T)).T\n",
        "        x_new, layers5 = self.conv5(x, adj, self.X_0)\n",
        "        x = x_new.T + self.V(x.T)\n",
        "        return x, layers1 + layers2 + layers3 + layers4 + layers5\n",
        "\n",
        "\n",
        "class PPRGNN_GC(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, out_dim, dropout, eps, **kwargs):\n",
        "        super(PPRGNN_GC, self).__init__()\n",
        "\n",
        "        self.conv1 = PPRLayer(in_dim, h_dim, eps=eps)\n",
        "        self.conv2 = PPRLayer(h_dim, h_dim, eps=eps)\n",
        "        self.conv3 = PPRLayer(h_dim, h_dim, eps=eps)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.X_0 = None\n",
        "        self.V_0 = nn.Linear(h_dim, h_dim)\n",
        "        self.V_1 = nn.Linear(h_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, adj, batch):\n",
        "        x, layers1 = self.conv1(x, adj, self.X_0)\n",
        "        x, layers2 = self.conv2(x, adj, self.X_0)\n",
        "        x, layers3 = self.conv3(x, adj, self.X_0)\n",
        "        x = x.T\n",
        "\n",
        "        x = global_add_pool(x, batch)\n",
        "        x = F.relu(self.V_0(x))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.V_1(x)\n",
        "        return F.log_softmax(x, dim=1), layers1 + layers2 + layers3\n",
        "\n",
        "\n",
        "class APPNP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, h_dim):\n",
        "        super(APPNP, self).__init__()\n",
        "        self.lin1 = pyg.nn.GCNConv(in_dim, h_dim, normalize=False)\n",
        "        self.lin2 = pyg.nn.GCNConv(h_dim, out_dim, normalize=False)\n",
        "        self.appnp = pyg.nn.APPNP(K=10, alpha=0.1, dropout=0, normalize=False)\n",
        "\n",
        "    def forward(self, features, adj=None, edge_index=None, edge_weight=None):\n",
        "        if adj is not None:\n",
        "            edge_index, edge_weight = adj.coalesce().indices(), adj.coalesce().values()\n",
        "        x = self.lin1(features, edge_index, edge_weight)\n",
        "        x = self.lin2(x, edge_index, edge_weight)\n",
        "        x = self.appnp(x, edge_index, edge_weight)\n",
        "        return x, 10\n",
        "\n",
        "\n",
        "class APPNP_GC(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, out_dim, dropout, **kwargs):\n",
        "        super(APPNP_GC, self).__init__()\n",
        "\n",
        "        self.lin1 = nn.Linear(in_dim, h_dim)\n",
        "        self.lin2 = nn.Linear(h_dim, h_dim)\n",
        "        self.appnp = pyg.nn.APPNP(K=10, alpha=0.1, dropout=0, normalize=False)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.V_0 = nn.Linear(h_dim, h_dim)\n",
        "        self.V_1 = nn.Linear(h_dim, out_dim)\n",
        "\n",
        "    def forward(self, features, adj, batch):\n",
        "        x = features.T\n",
        "        edge_index, edge_weight = adj.coalesce().indices(), adj.coalesce().values()\n",
        "\n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.appnp(x, edge_index, edge_weight)\n",
        "\n",
        "        x = global_add_pool(x, batch)\n",
        "        x = F.relu(self.V_0(x))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.V_1(x)\n",
        "        return F.log_softmax(x, dim=1), 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5olZxmrdcq"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZMI6XfGsOKu"
      },
      "outputs": [],
      "source": [
        "# Train Utils\n",
        "\n",
        "def init_seeds(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm-E9goWp2O2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def Evaluation(output, labels):\n",
        "    preds = output.cpu().detach().numpy()\n",
        "    labels = labels.cpu().detach().numpy()\n",
        "    num_correct = 0\n",
        "    binary_pred = np.zeros(preds.shape).astype('int')\n",
        "    for i in range(preds.shape[0]):\n",
        "        k = labels[i].sum().astype('int')\n",
        "        topk_idx = preds[i].argsort()[-k:]\n",
        "        binary_pred[i][topk_idx] = 1\n",
        "        for pos in list(labels[i].nonzero()[0]):\n",
        "            if labels[i][pos] and labels[i][pos] == binary_pred[i][pos]:\n",
        "                num_correct += 1\n",
        "\n",
        "    print('total number of correct is: {}'.format(num_correct))\n",
        "\n",
        "    return (f1_score(labels, binary_pred, average=\"micro\"), f1_score(labels, binary_pred, average=\"macro\"))\n",
        "'''\n",
        "\n",
        "def Evaluation(output, labels, threshold=0.5):\n",
        "    # Apply sigmoid to logits\n",
        "    probs = torch.sigmoid(output)\n",
        "    # Binarize predictions\n",
        "    preds = (probs > threshold).float()\n",
        "\n",
        "    y_true = labels.cpu().numpy()\n",
        "    y_pred = preds.cpu().numpy()\n",
        "\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    return f1_micro, f1_macro\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device, clip):\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    layers = []\n",
        "    f1_scores = []\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "        output, layers_used = model(batch.x, batch.edge_index, batch.edge_weight)\n",
        "        loss = criterion(output, batch.y)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        if clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        predictions = (output > 0.5)\n",
        "        loss_list.append(loss.item())\n",
        "        layers.append(layers_used)\n",
        "        f1_scores.append(f1_score(batch.y.cpu().numpy(), predictions.cpu().numpy(), average='micro'))\n",
        "    return np.mean(f1_scores), np.mean(loss_list), np.mean(layers)\n",
        "\n",
        "\n",
        "def test(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss_list = []\n",
        "        layers = []\n",
        "        f1_scores = []\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "            output, layers_used = model(batch.x, batch.edge_index, batch.edge_weight)\n",
        "            loss = criterion(output, batch.y)\n",
        "            predictions = (output > 0.5)\n",
        "            loss_list.append(loss.item())\n",
        "            layers.append(layers_used)\n",
        "            f1_scores.append(f1_score(batch.y.cpu().numpy(), predictions.cpu().numpy(), average='micro'))\n",
        "        return np.mean(f1_scores), np.mean(loss_list), np.mean(layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_distribution(labels, idx_train, portion):\n",
        "    # Get the dominant (max-score) class for each training sample\n",
        "    dominant_labels = torch.argmax(labels[idx_train], dim=1).cpu().numpy()\n",
        "\n",
        "    # Count occurrences\n",
        "    label_counts = Counter(dominant_labels)\n",
        "\n",
        "    print(f\" Train label distribution (portion={portion}):\")\n",
        "    for label, count in sorted(label_counts.items()):\n",
        "        print(f\"  Class {label}: {count} samples\")\n",
        "    print(f\"  Total classes in train set: {len(label_counts)} / {labels.shape[1]}\")\n",
        "    print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "-bqMpgBM5jrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQb66dWTre1u"
      },
      "outputs": [],
      "source": [
        "def amazon_train_model(lr, weight_decay, self_loops, epochs, clip,\n",
        "                width, portion, schedule, **kwargs):\n",
        "    init_seeds()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    adj, features, labels, idx_train, idx_val, _, num_nodes, num_class = load_txt_data( dataset_str=\"amazon-all\",\n",
        "    portion=str(portion),\n",
        "    self_loops=True,\n",
        "    root=root)\n",
        "\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    check_distribution(labels, idx_train, portion)\n",
        "    idx_val = idx_val.cuda()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    if kwargs[\"type\"] == 'appnp':\n",
        "        model = APPNP(in_dim=features.shape[1], out_dim=num_class, h_dim=width)\n",
        "    elif kwargs[\"type\"] == \"pprgnn\":\n",
        "        model = PPRGNN_Amazon(in_dim=features.shape[1], out_dim=num_class, num_nodes=num_nodes, h_dim=width,\n",
        "                              **kwargs)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    if schedule:\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4, verbose=True, factor=0.9)\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    start_epoch = 1\n",
        "    best_f1_micro = 0\n",
        "    min_test_loss = np.inf\n",
        "\n",
        "    best_loss = 0\n",
        "    best_f1_macro = 0\n",
        "\n",
        "    not_improved_for = 0\n",
        "    start_time = time.time()\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        epoch_start = time.time()\n",
        "        output_string = f'Epoch {epoch}:'\n",
        "        # train\n",
        "        model.train()\n",
        "        output, layers_train = model(features, adj=adj)\n",
        "        train_loss = criterion(output[idx_train], labels[idx_train])\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        if clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        f1_train_micro, f1_train_macro = Evaluation(output[idx_train], labels[idx_train])\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output, layers_test = model(features, adj=adj)\n",
        "\n",
        "            test_loss = criterion(output[idx_val], labels[idx_val])\n",
        "            test_f1_micro, f1_test_macro = Evaluation(output[idx_val], labels[idx_val])\n",
        "\n",
        "        output_string += f' Train: Loss: {train_loss:.4f}, Micro_F1: {f1_train_micro:.4f}, Macro_F1: {f1_train_macro:.4f}, Layers: {layers_train},' \\\n",
        "                         f' Test: Loss: {test_loss:.4f}, Micro_F1: {test_f1_micro:.4f}, Macro_F1: {f1_test_macro:.4f}, Layers: {layers_test}'\n",
        "\n",
        "        output_string += f' - ({time.time() - epoch_start:.4f}s)'\n",
        "        print(output_string)\n",
        "        if test_f1_micro > best_f1_micro:\n",
        "            print(\"------------ This was a new best! ----------\")\n",
        "            best_f1_micro = test_f1_micro\n",
        "            best_f1_macro = f1_test_macro\n",
        "            best_loss = test_loss.item()\n",
        "            not_improved_for = 0\n",
        "            time_to_best = time.time() - start_time\n",
        "            best_epoch = epoch\n",
        "        else:\n",
        "            if test_loss > min_test_loss:\n",
        "                not_improved_for += 1\n",
        "                if not_improved_for == 100:\n",
        "                    break\n",
        "            else:\n",
        "                not_improved_for = 0\n",
        "                min_test_loss = test_loss\n",
        "        if schedule:\n",
        "            scheduler.step(train_loss)\n",
        "    return {\n",
        "        'f1_micro': best_f1_micro,\n",
        "        'f1_macro': best_f1_macro,\n",
        "        'loss': best_loss,\n",
        "        'total_time': time.time() - start_time,\n",
        "        'time_to_best': time_to_best,\n",
        "        'total_iterations': epoch,\n",
        "        'best_epoch': best_epoch,\n",
        "        'time_per_iteration': time_to_best / best_epoch\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full path to pprgnn.json\n",
        "pprgnn_config_path = root + 'pprgnn.json'\n",
        "\n",
        "# Full path to appnp.json\n",
        "appnp_config_path = root + 'appnp.json'"
      ],
      "metadata": {
        "id": "fiWZDFdGcMls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_config = True\n",
        "dataset = 'Amazon'\n",
        "for portion in [0.05, 0.06, 0.07, 0.08, 0.09]:\n",
        "    params = {\n",
        "        'dataset': dataset,\n",
        "        'epochs': 10,\n",
        "        'self_loops': False,\n",
        "        'type': 'pprgnn',\n",
        "        'eps': 0.06,\n",
        "        'lr': 0.01,\n",
        "        'clip': 1.0,\n",
        "        'weight_decay': 1e-8,\n",
        "        'dropout': 0.5,\n",
        "        'width': 128,\n",
        "        'schedule': True,\n",
        "        'portion': portion\n",
        "    }\n",
        "    if load_config:\n",
        "        with open(pprgnn_config_path) as f:\n",
        "            loaded_config = json.load(f)\n",
        "        params = params | loaded_config\n",
        "\n",
        "    metrics = amazon_train_model(**params)\n",
        "\n",
        "    print(f'Results for {params[\"type\"]}: {metrics}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz1SbP1oBtYF",
        "outputId": "761e23f3-d43c-499b-c879-32e93beb7a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1133913501.py:8: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:644.)\n",
            "  tensor = torch.sparse.FloatTensor(indices, values, shape)\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.05):\n",
            "  Class 0: 308 samples\n",
            "  Class 1: 1027 samples\n",
            "  Class 3: 54 samples\n",
            "  Class 4: 951 samples\n",
            "  Class 5: 314 samples\n",
            "  Class 6: 1163 samples\n",
            "  Class 7: 239 samples\n",
            "  Class 8: 221 samples\n",
            "  Class 11: 448 samples\n",
            "  Class 12: 934 samples\n",
            "  Class 13: 242 samples\n",
            "  Class 14: 520 samples\n",
            "  Class 15: 352 samples\n",
            "  Class 16: 3 samples\n",
            "  Class 18: 1324 samples\n",
            "  Class 19: 166 samples\n",
            "  Class 24: 541 samples\n",
            "  Class 25: 309 samples\n",
            "  Class 26: 171 samples\n",
            "  Class 27: 109 samples\n",
            "  Class 29: 472 samples\n",
            "  Class 34: 230 samples\n",
            "  Class 42: 269 samples\n",
            "  Class 43: 228 samples\n",
            "  Class 45: 310 samples\n",
            "  Class 46: 251 samples\n",
            "  Class 47: 289 samples\n",
            "  Class 48: 465 samples\n",
            "  Class 50: 402 samples\n",
            "  Class 51: 183 samples\n",
            "  Class 52: 1339 samples\n",
            "  Class 54: 308 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train: Loss: 0.6928, Micro_F1: 0.0860, Macro_F1: 0.0789, Layers: 9, Test: Loss: 0.6837, Micro_F1: 0.0527, Macro_F1: 0.0245, Layers: 16 - (7.7509s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6828, Micro_F1: 0.0820, Macro_F1: 0.0580, Layers: 16, Test: Loss: 0.5924, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 31 - (10.3072s)\n",
            "Epoch 3: Train: Loss: 0.5758, Micro_F1: 0.0052, Macro_F1: 0.0037, Layers: 31, Test: Loss: 1.2667, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 52 - (16.6732s)\n",
            "Epoch 4: Train: Loss: 1.4884, Micro_F1: 0.0001, Macro_F1: 0.0002, Layers: 52, Test: Loss: 0.2868, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 39 - (18.0874s)\n",
            "Epoch 5: Train: Loss: 0.2624, Micro_F1: 0.0014, Macro_F1: 0.0007, Layers: 39, Test: Loss: 0.4038, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 36 - (15.4338s)\n",
            "Epoch 6: Train: Loss: 0.3458, Micro_F1: 0.0047, Macro_F1: 0.0018, Layers: 36, Test: Loss: 0.2388, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 40 - (15.6128s)\n",
            "Epoch 7: Train: Loss: 0.2163, Micro_F1: 0.0088, Macro_F1: 0.0020, Layers: 40, Test: Loss: 0.3471, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 46 - (17.2874s)\n",
            "Epoch 8: Train: Loss: 0.4465, Micro_F1: 0.0154, Macro_F1: 0.0029, Layers: 46, Test: Loss: 0.2336, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 40 - (17.2952s)\n",
            "Epoch 9: Train: Loss: 0.2082, Micro_F1: 0.0503, Macro_F1: 0.0091, Layers: 40, Test: Loss: 0.3773, Micro_F1: 0.0013, Macro_F1: 0.0003, Layers: 35 - (15.3510s)\n",
            "Results for pprgnn: {'f1_micro': 0.0527485482599033, 'f1_macro': 0.024463287892766125, 'loss': 0.6837331652641296, 'total_time': 133.80317974090576, 'time_to_best': 7.751294374465942, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 7.751294374465942}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.06):\n",
            "  Class 0: 364 samples\n",
            "  Class 1: 1234 samples\n",
            "  Class 3: 66 samples\n",
            "  Class 4: 1151 samples\n",
            "  Class 5: 385 samples\n",
            "  Class 6: 1380 samples\n",
            "  Class 7: 281 samples\n",
            "  Class 8: 252 samples\n",
            "  Class 11: 531 samples\n",
            "  Class 12: 1121 samples\n",
            "  Class 13: 295 samples\n",
            "  Class 14: 613 samples\n",
            "  Class 15: 418 samples\n",
            "  Class 16: 3 samples\n",
            "  Class 18: 1605 samples\n",
            "  Class 19: 205 samples\n",
            "  Class 24: 651 samples\n",
            "  Class 25: 378 samples\n",
            "  Class 26: 208 samples\n",
            "  Class 27: 122 samples\n",
            "  Class 29: 566 samples\n",
            "  Class 34: 281 samples\n",
            "  Class 42: 320 samples\n",
            "  Class 43: 288 samples\n",
            "  Class 45: 386 samples\n",
            "  Class 46: 308 samples\n",
            "  Class 47: 343 samples\n",
            "  Class 48: 572 samples\n",
            "  Class 50: 470 samples\n",
            "  Class 51: 211 samples\n",
            "  Class 52: 1606 samples\n",
            "  Class 54: 356 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train: Loss: 0.6928, Micro_F1: 0.0860, Macro_F1: 0.0790, Layers: 9, Test: Loss: 0.6837, Micro_F1: 0.0530, Macro_F1: 0.0246, Layers: 16 - (6.7977s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6828, Micro_F1: 0.0821, Macro_F1: 0.0583, Layers: 16, Test: Loss: 0.5902, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 31 - (10.4973s)\n",
            "Epoch 3: Train: Loss: 0.5744, Micro_F1: 0.0054, Macro_F1: 0.0037, Layers: 31, Test: Loss: 1.3296, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 52 - (16.9218s)\n",
            "Epoch 4: Train: Loss: 1.5463, Micro_F1: 0.0001, Macro_F1: 0.0001, Layers: 52, Test: Loss: 0.2775, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 39 - (18.3160s)\n",
            "Epoch 5: Train: Loss: 0.2576, Micro_F1: 0.0012, Macro_F1: 0.0006, Layers: 39, Test: Loss: 0.3996, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 36 - (15.4361s)\n",
            "Epoch 6: Train: Loss: 0.3464, Micro_F1: 0.0044, Macro_F1: 0.0017, Layers: 36, Test: Loss: 0.2399, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 40 - (15.5136s)\n",
            "Epoch 7: Train: Loss: 0.2177, Micro_F1: 0.0092, Macro_F1: 0.0021, Layers: 40, Test: Loss: 0.3530, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 46 - (17.3408s)\n",
            "Epoch 8: Train: Loss: 0.4438, Micro_F1: 0.0158, Macro_F1: 0.0030, Layers: 46, Test: Loss: 0.2261, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 40 - (17.3965s)\n",
            "Epoch 9: Train: Loss: 0.2071, Micro_F1: 0.0499, Macro_F1: 0.0091, Layers: 40, Test: Loss: 0.3680, Micro_F1: 0.0011, Macro_F1: 0.0003, Layers: 35 - (15.4050s)\n",
            "Results for pprgnn: {'f1_micro': 0.05301143258606374, 'f1_macro': 0.02458319936326608, 'loss': 0.6836985349655151, 'total_time': 133.62858748435974, 'time_to_best': 6.79779052734375, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 6.79779052734375}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.07):\n",
            "  Class 0: 419 samples\n",
            "  Class 1: 1427 samples\n",
            "  Class 3: 80 samples\n",
            "  Class 4: 1324 samples\n",
            "  Class 5: 441 samples\n",
            "  Class 6: 1632 samples\n",
            "  Class 7: 337 samples\n",
            "  Class 8: 285 samples\n",
            "  Class 11: 635 samples\n",
            "  Class 12: 1289 samples\n",
            "  Class 13: 346 samples\n",
            "  Class 14: 719 samples\n",
            "  Class 15: 473 samples\n",
            "  Class 16: 3 samples\n",
            "  Class 18: 1886 samples\n",
            "  Class 19: 239 samples\n",
            "  Class 24: 780 samples\n",
            "  Class 25: 446 samples\n",
            "  Class 26: 234 samples\n",
            "  Class 27: 142 samples\n",
            "  Class 29: 658 samples\n",
            "  Class 34: 317 samples\n",
            "  Class 42: 377 samples\n",
            "  Class 43: 338 samples\n",
            "  Class 45: 433 samples\n",
            "  Class 46: 366 samples\n",
            "  Class 47: 410 samples\n",
            "  Class 48: 675 samples\n",
            "  Class 50: 540 samples\n",
            "  Class 51: 243 samples\n",
            "  Class 52: 1894 samples\n",
            "  Class 54: 411 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train: Loss: 0.6928, Micro_F1: 0.0858, Macro_F1: 0.0789, Layers: 9, Test: Loss: 0.6836, Micro_F1: 0.0538, Macro_F1: 0.0248, Layers: 16 - (6.7108s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6827, Micro_F1: 0.0823, Macro_F1: 0.0583, Layers: 16, Test: Loss: 0.5865, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 32 - (10.7426s)\n",
            "Epoch 3: Train: Loss: 0.5711, Micro_F1: 0.0052, Macro_F1: 0.0036, Layers: 32, Test: Loss: 1.4040, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 52 - (17.1522s)\n",
            "Epoch 4: Train: Loss: 1.6124, Micro_F1: 0.0001, Macro_F1: 0.0001, Layers: 52, Test: Loss: 0.2625, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 39 - (18.2348s)\n",
            "Epoch 5: Train: Loss: 0.2482, Micro_F1: 0.0013, Macro_F1: 0.0006, Layers: 39, Test: Loss: 0.3961, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 36 - (15.4272s)\n",
            "Epoch 6: Train: Loss: 0.3466, Micro_F1: 0.0051, Macro_F1: 0.0018, Layers: 36, Test: Loss: 0.2458, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 39 - (15.3616s)\n",
            "Epoch 7: Train: Loss: 0.2213, Micro_F1: 0.0100, Macro_F1: 0.0023, Layers: 39, Test: Loss: 0.3426, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 46 - (17.2093s)\n",
            "Epoch 8: Train: Loss: 0.4212, Micro_F1: 0.0166, Macro_F1: 0.0031, Layers: 46, Test: Loss: 0.2145, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 40 - (17.3293s)\n",
            "Epoch 9: Train: Loss: 0.2052, Micro_F1: 0.0501, Macro_F1: 0.0092, Layers: 40, Test: Loss: 0.3441, Micro_F1: 0.0006, Macro_F1: 0.0001, Layers: 35 - (15.4424s)\n",
            "Results for pprgnn: {'f1_micro': 0.05383998717332051, 'f1_macro': 0.024844754163684517, 'loss': 0.6836146712303162, 'total_time': 133.6146628856659, 'time_to_best': 6.710885047912598, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 6.710885047912598}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.08):\n",
            "  Class 0: 484 samples\n",
            "  Class 1: 1619 samples\n",
            "  Class 3: 90 samples\n",
            "  Class 4: 1481 samples\n",
            "  Class 5: 492 samples\n",
            "  Class 6: 1853 samples\n",
            "  Class 7: 387 samples\n",
            "  Class 8: 334 samples\n",
            "  Class 11: 725 samples\n",
            "  Class 12: 1464 samples\n",
            "  Class 13: 407 samples\n",
            "  Class 14: 828 samples\n",
            "  Class 15: 542 samples\n",
            "  Class 16: 4 samples\n",
            "  Class 18: 2187 samples\n",
            "  Class 19: 281 samples\n",
            "  Class 24: 877 samples\n",
            "  Class 25: 508 samples\n",
            "  Class 26: 261 samples\n",
            "  Class 27: 155 samples\n",
            "  Class 29: 757 samples\n",
            "  Class 34: 346 samples\n",
            "  Class 42: 437 samples\n",
            "  Class 43: 393 samples\n",
            "  Class 45: 494 samples\n",
            "  Class 46: 420 samples\n",
            "  Class 47: 475 samples\n",
            "  Class 48: 771 samples\n",
            "  Class 50: 627 samples\n",
            "  Class 51: 282 samples\n",
            "  Class 52: 2195 samples\n",
            "  Class 54: 451 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train: Loss: 0.6928, Micro_F1: 0.0860, Macro_F1: 0.0791, Layers: 9, Test: Loss: 0.6836, Micro_F1: 0.0543, Macro_F1: 0.0249, Layers: 16 - (6.8513s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6827, Micro_F1: 0.0821, Macro_F1: 0.0580, Layers: 16, Test: Loss: 0.5834, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 32 - (10.7722s)\n",
            "Epoch 3: Train: Loss: 0.5688, Micro_F1: 0.0052, Macro_F1: 0.0036, Layers: 32, Test: Loss: 1.4886, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 52 - (17.1054s)\n",
            "Epoch 4: Train: Loss: 1.6938, Micro_F1: 0.0001, Macro_F1: 0.0001, Layers: 52, Test: Loss: 0.2531, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 39 - (18.2542s)\n",
            "Epoch 5: Train: Loss: 0.2430, Micro_F1: 0.0013, Macro_F1: 0.0006, Layers: 39, Test: Loss: 0.3947, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 35 - (15.2912s)\n",
            "Epoch 6: Train: Loss: 0.3491, Micro_F1: 0.0054, Macro_F1: 0.0018, Layers: 35, Test: Loss: 0.2508, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 39 - (15.2357s)\n",
            "Epoch 7: Train: Loss: 0.2259, Micro_F1: 0.0109, Macro_F1: 0.0024, Layers: 39, Test: Loss: 0.3356, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 45 - (16.9875s)\n",
            "Epoch 8: Train: Loss: 0.4064, Micro_F1: 0.0172, Macro_F1: 0.0031, Layers: 45, Test: Loss: 0.2076, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 40 - (17.1854s)\n",
            "Epoch 9: Train: Loss: 0.2058, Micro_F1: 0.0490, Macro_F1: 0.0090, Layers: 40, Test: Loss: 0.3275, Micro_F1: 0.0003, Macro_F1: 0.0001, Layers: 36 - (15.7056s)\n",
            "Results for pprgnn: {'f1_micro': 0.054296677043364945, 'f1_macro': 0.024907834388672848, 'loss': 0.6835511922836304, 'total_time': 133.39277696609497, 'time_to_best': 6.851606607437134, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 6.851606607437134}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.09):\n",
            "  Class 0: 546 samples\n",
            "  Class 1: 1809 samples\n",
            "  Class 3: 101 samples\n",
            "  Class 4: 1681 samples\n",
            "  Class 5: 555 samples\n",
            "  Class 6: 2111 samples\n",
            "  Class 7: 441 samples\n",
            "  Class 8: 375 samples\n",
            "  Class 11: 833 samples\n",
            "  Class 12: 1639 samples\n",
            "  Class 13: 466 samples\n",
            "  Class 14: 929 samples\n",
            "  Class 15: 590 samples\n",
            "  Class 16: 4 samples\n",
            "  Class 18: 2452 samples\n",
            "  Class 19: 308 samples\n",
            "  Class 24: 970 samples\n",
            "  Class 25: 573 samples\n",
            "  Class 26: 296 samples\n",
            "  Class 27: 173 samples\n",
            "  Class 29: 850 samples\n",
            "  Class 34: 385 samples\n",
            "  Class 42: 491 samples\n",
            "  Class 43: 438 samples\n",
            "  Class 45: 547 samples\n",
            "  Class 46: 485 samples\n",
            "  Class 47: 526 samples\n",
            "  Class 48: 890 samples\n",
            "  Class 50: 703 samples\n",
            "  Class 51: 305 samples\n",
            "  Class 52: 2465 samples\n",
            "  Class 54: 519 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train: Loss: 0.6928, Micro_F1: 0.0855, Macro_F1: 0.0787, Layers: 9, Test: Loss: 0.6835, Micro_F1: 0.0548, Macro_F1: 0.0251, Layers: 16 - (6.7671s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6827, Micro_F1: 0.0823, Macro_F1: 0.0582, Layers: 16, Test: Loss: 0.5810, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 32 - (10.7856s)\n",
            "Epoch 3: Train: Loss: 0.5670, Micro_F1: 0.0052, Macro_F1: 0.0036, Layers: 32, Test: Loss: 1.5453, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 52 - (17.1728s)\n",
            "Epoch 4: Train: Loss: 1.7349, Micro_F1: 0.0001, Macro_F1: 0.0001, Layers: 52, Test: Loss: 0.2455, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 39 - (18.2834s)\n",
            "Epoch 5: Train: Loss: 0.2382, Micro_F1: 0.0013, Macro_F1: 0.0006, Layers: 39, Test: Loss: 0.3932, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 35 - (15.3087s)\n",
            "Epoch 6: Train: Loss: 0.3505, Micro_F1: 0.0059, Macro_F1: 0.0019, Layers: 35, Test: Loss: 0.2535, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 38 - (15.1364s)\n",
            "Epoch 7: Train: Loss: 0.2289, Micro_F1: 0.0118, Macro_F1: 0.0026, Layers: 38, Test: Loss: 0.3353, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 45 - (16.8949s)\n",
            "Epoch 8: Train: Loss: 0.3988, Micro_F1: 0.0182, Macro_F1: 0.0033, Layers: 45, Test: Loss: 0.2036, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 40 - (17.1921s)\n",
            "Epoch 9: Train: Loss: 0.2060, Micro_F1: 0.0481, Macro_F1: 0.0090, Layers: 40, Test: Loss: 0.3144, Micro_F1: 0.0002, Macro_F1: 0.0000, Layers: 36 - (15.7152s)\n",
            "Results for pprgnn: {'f1_micro': 0.05483605631985549, 'f1_macro': 0.025091300191825105, 'loss': 0.6835018992424011, 'total_time': 133.26034688949585, 'time_to_best': 6.76717472076416, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 6.76717472076416}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqUe8VVQtW8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870dbeec-9fb7-4a30-8bc2-5f61bd60beea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.05):\n",
            "  Class 0: 308 samples\n",
            "  Class 1: 1027 samples\n",
            "  Class 3: 54 samples\n",
            "  Class 4: 951 samples\n",
            "  Class 5: 314 samples\n",
            "  Class 6: 1163 samples\n",
            "  Class 7: 239 samples\n",
            "  Class 8: 221 samples\n",
            "  Class 11: 448 samples\n",
            "  Class 12: 934 samples\n",
            "  Class 13: 242 samples\n",
            "  Class 14: 520 samples\n",
            "  Class 15: 352 samples\n",
            "  Class 16: 3 samples\n",
            "  Class 18: 1324 samples\n",
            "  Class 19: 166 samples\n",
            "  Class 24: 541 samples\n",
            "  Class 25: 309 samples\n",
            "  Class 26: 171 samples\n",
            "  Class 27: 109 samples\n",
            "  Class 29: 472 samples\n",
            "  Class 34: 230 samples\n",
            "  Class 42: 269 samples\n",
            "  Class 43: 228 samples\n",
            "  Class 45: 310 samples\n",
            "  Class 46: 251 samples\n",
            "  Class 47: 289 samples\n",
            "  Class 48: 465 samples\n",
            "  Class 50: 402 samples\n",
            "  Class 51: 183 samples\n",
            "  Class 52: 1339 samples\n",
            "  Class 54: 308 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n",
            "Epoch 1: Train: Loss: 0.6931, Micro_F1: 0.0839, Macro_F1: 0.0802, Layers: 10, Test: Loss: 0.6808, Micro_F1: 0.1418, Macro_F1: 0.1096, Layers: 10 - (1.0956s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6801, Micro_F1: 0.2149, Macro_F1: 0.1561, Layers: 10, Test: Loss: 0.6587, Micro_F1: 0.0011, Macro_F1: 0.0021, Layers: 10 - (1.0002s)\n",
            "Epoch 3: Train: Loss: 0.6567, Micro_F1: 0.0040, Macro_F1: 0.0066, Layers: 10, Test: Loss: 0.6265, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (0.9981s)\n",
            "Epoch 4: Train: Loss: 0.6225, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5838, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0852s)\n",
            "Epoch 5: Train: Loss: 0.5774, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5319, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.1169s)\n",
            "Epoch 6: Train: Loss: 0.5230, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4737, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.1136s)\n",
            "Epoch 7: Train: Loss: 0.4624, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4128, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (0.9903s)\n",
            "Epoch 8: Train: Loss: 0.4000, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.3539, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (0.9924s)\n",
            "Epoch 9: Train: Loss: 0.3406, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.3014, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (0.9968s)\n",
            "Results for appnp: {'f1_micro': 0.14183088517316803, 'f1_macro': 0.10959964147542621, 'loss': 0.6808226704597473, 'total_time': 9.392827987670898, 'time_to_best': 1.095724105834961, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 1.095724105834961}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.06):\n",
            "  Class 0: 364 samples\n",
            "  Class 1: 1234 samples\n",
            "  Class 3: 66 samples\n",
            "  Class 4: 1151 samples\n",
            "  Class 5: 385 samples\n",
            "  Class 6: 1380 samples\n",
            "  Class 7: 281 samples\n",
            "  Class 8: 252 samples\n",
            "  Class 11: 531 samples\n",
            "  Class 12: 1121 samples\n",
            "  Class 13: 295 samples\n",
            "  Class 14: 613 samples\n",
            "  Class 15: 418 samples\n",
            "  Class 16: 3 samples\n",
            "  Class 18: 1605 samples\n",
            "  Class 19: 205 samples\n",
            "  Class 24: 651 samples\n",
            "  Class 25: 378 samples\n",
            "  Class 26: 208 samples\n",
            "  Class 27: 122 samples\n",
            "  Class 29: 566 samples\n",
            "  Class 34: 281 samples\n",
            "  Class 42: 320 samples\n",
            "  Class 43: 288 samples\n",
            "  Class 45: 386 samples\n",
            "  Class 46: 308 samples\n",
            "  Class 47: 343 samples\n",
            "  Class 48: 572 samples\n",
            "  Class 50: 470 samples\n",
            "  Class 51: 211 samples\n",
            "  Class 52: 1606 samples\n",
            "  Class 54: 356 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n",
            "Epoch 1: Train: Loss: 0.6931, Micro_F1: 0.0839, Macro_F1: 0.0802, Layers: 10, Test: Loss: 0.6807, Micro_F1: 0.1510, Macro_F1: 0.1117, Layers: 10 - (1.1405s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6800, Micro_F1: 0.2175, Macro_F1: 0.1501, Layers: 10, Test: Loss: 0.6583, Micro_F1: 0.0010, Macro_F1: 0.0019, Layers: 10 - (1.0335s)\n",
            "Epoch 3: Train: Loss: 0.6564, Micro_F1: 0.0031, Macro_F1: 0.0053, Layers: 10, Test: Loss: 0.6256, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0310s)\n",
            "Epoch 4: Train: Loss: 0.6221, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5824, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0270s)\n",
            "Epoch 5: Train: Loss: 0.5768, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5301, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0274s)\n",
            "Epoch 6: Train: Loss: 0.5222, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4714, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0276s)\n",
            "Epoch 7: Train: Loss: 0.4614, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4102, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0289s)\n",
            "Epoch 8: Train: Loss: 0.3990, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.3512, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.1496s)\n",
            "Epoch 9: Train: Loss: 0.3397, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.2989, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.1437s)\n",
            "Results for appnp: {'f1_micro': 0.1509531224829512, 'f1_macro': 0.11168704712757323, 'loss': 0.6806545853614807, 'total_time': 9.612675428390503, 'time_to_best': 1.1406126022338867, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 1.1406126022338867}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.07):\n",
            "  Class 0: 419 samples\n",
            "  Class 1: 1427 samples\n",
            "  Class 3: 80 samples\n",
            "  Class 4: 1324 samples\n",
            "  Class 5: 441 samples\n",
            "  Class 6: 1632 samples\n",
            "  Class 7: 337 samples\n",
            "  Class 8: 285 samples\n",
            "  Class 11: 635 samples\n",
            "  Class 12: 1289 samples\n",
            "  Class 13: 346 samples\n",
            "  Class 14: 719 samples\n",
            "  Class 15: 473 samples\n",
            "  Class 16: 3 samples\n",
            "  Class 18: 1886 samples\n",
            "  Class 19: 239 samples\n",
            "  Class 24: 780 samples\n",
            "  Class 25: 446 samples\n",
            "  Class 26: 234 samples\n",
            "  Class 27: 142 samples\n",
            "  Class 29: 658 samples\n",
            "  Class 34: 317 samples\n",
            "  Class 42: 377 samples\n",
            "  Class 43: 338 samples\n",
            "  Class 45: 433 samples\n",
            "  Class 46: 366 samples\n",
            "  Class 47: 410 samples\n",
            "  Class 48: 675 samples\n",
            "  Class 50: 540 samples\n",
            "  Class 51: 243 samples\n",
            "  Class 52: 1894 samples\n",
            "  Class 54: 411 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n",
            "Epoch 1: Train: Loss: 0.6931, Micro_F1: 0.0838, Macro_F1: 0.0801, Layers: 10, Test: Loss: 0.6806, Micro_F1: 0.1554, Macro_F1: 0.1125, Layers: 10 - (1.1525s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6800, Micro_F1: 0.2145, Macro_F1: 0.1469, Layers: 10, Test: Loss: 0.6581, Micro_F1: 0.0047, Macro_F1: 0.0078, Layers: 10 - (1.0394s)\n",
            "Epoch 3: Train: Loss: 0.6564, Micro_F1: 0.0100, Macro_F1: 0.0133, Layers: 10, Test: Loss: 0.6253, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0295s)\n",
            "Epoch 4: Train: Loss: 0.6221, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5819, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0262s)\n",
            "Epoch 5: Train: Loss: 0.5768, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5294, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0311s)\n",
            "Epoch 6: Train: Loss: 0.5222, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4705, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0337s)\n",
            "Epoch 7: Train: Loss: 0.4615, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4092, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0319s)\n",
            "Epoch 8: Train: Loss: 0.3990, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.3502, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0220s)\n",
            "Epoch 9: Train: Loss: 0.3397, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.2979, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0250s)\n",
            "Results for appnp: {'f1_micro': 0.15539320831652048, 'f1_macro': 0.11249113369814874, 'loss': 0.6805973649024963, 'total_time': 9.394602298736572, 'time_to_best': 1.1525893211364746, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 1.1525893211364746}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.08):\n",
            "  Class 0: 484 samples\n",
            "  Class 1: 1619 samples\n",
            "  Class 3: 90 samples\n",
            "  Class 4: 1481 samples\n",
            "  Class 5: 492 samples\n",
            "  Class 6: 1853 samples\n",
            "  Class 7: 387 samples\n",
            "  Class 8: 334 samples\n",
            "  Class 11: 725 samples\n",
            "  Class 12: 1464 samples\n",
            "  Class 13: 407 samples\n",
            "  Class 14: 828 samples\n",
            "  Class 15: 542 samples\n",
            "  Class 16: 4 samples\n",
            "  Class 18: 2187 samples\n",
            "  Class 19: 281 samples\n",
            "  Class 24: 877 samples\n",
            "  Class 25: 508 samples\n",
            "  Class 26: 261 samples\n",
            "  Class 27: 155 samples\n",
            "  Class 29: 757 samples\n",
            "  Class 34: 346 samples\n",
            "  Class 42: 437 samples\n",
            "  Class 43: 393 samples\n",
            "  Class 45: 494 samples\n",
            "  Class 46: 420 samples\n",
            "  Class 47: 475 samples\n",
            "  Class 48: 771 samples\n",
            "  Class 50: 627 samples\n",
            "  Class 51: 282 samples\n",
            "  Class 52: 2195 samples\n",
            "  Class 54: 451 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n",
            "Epoch 1: Train: Loss: 0.6931, Micro_F1: 0.0841, Macro_F1: 0.0803, Layers: 10, Test: Loss: 0.6805, Micro_F1: 0.1640, Macro_F1: 0.1141, Layers: 10 - (1.1886s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6800, Micro_F1: 0.2171, Macro_F1: 0.1455, Layers: 10, Test: Loss: 0.6579, Micro_F1: 0.0005, Macro_F1: 0.0009, Layers: 10 - (1.2104s)\n",
            "Epoch 3: Train: Loss: 0.6565, Micro_F1: 0.0014, Macro_F1: 0.0026, Layers: 10, Test: Loss: 0.6248, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.1771s)\n",
            "Epoch 4: Train: Loss: 0.6220, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5811, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0964s)\n",
            "Epoch 5: Train: Loss: 0.5766, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5283, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0385s)\n",
            "Epoch 6: Train: Loss: 0.5220, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4692, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0473s)\n",
            "Epoch 7: Train: Loss: 0.4613, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4078, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0386s)\n",
            "Epoch 8: Train: Loss: 0.3989, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.3488, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0372s)\n",
            "Epoch 9: Train: Loss: 0.3397, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.2967, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0365s)\n",
            "Results for appnp: {'f1_micro': 0.16402938984133744, 'f1_macro': 0.1141438383364431, 'loss': 0.680542528629303, 'total_time': 9.874123334884644, 'time_to_best': 1.1887273788452148, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 1.1887273788452148}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.,  2.,  2.,  ..., 11.,  9., 10.])\n",
            " Train label distribution (portion=0.09):\n",
            "  Class 0: 546 samples\n",
            "  Class 1: 1809 samples\n",
            "  Class 3: 101 samples\n",
            "  Class 4: 1681 samples\n",
            "  Class 5: 555 samples\n",
            "  Class 6: 2111 samples\n",
            "  Class 7: 441 samples\n",
            "  Class 8: 375 samples\n",
            "  Class 11: 833 samples\n",
            "  Class 12: 1639 samples\n",
            "  Class 13: 466 samples\n",
            "  Class 14: 929 samples\n",
            "  Class 15: 590 samples\n",
            "  Class 16: 4 samples\n",
            "  Class 18: 2452 samples\n",
            "  Class 19: 308 samples\n",
            "  Class 24: 970 samples\n",
            "  Class 25: 573 samples\n",
            "  Class 26: 296 samples\n",
            "  Class 27: 173 samples\n",
            "  Class 29: 850 samples\n",
            "  Class 34: 385 samples\n",
            "  Class 42: 491 samples\n",
            "  Class 43: 438 samples\n",
            "  Class 45: 547 samples\n",
            "  Class 46: 485 samples\n",
            "  Class 47: 526 samples\n",
            "  Class 48: 890 samples\n",
            "  Class 50: 703 samples\n",
            "  Class 51: 305 samples\n",
            "  Class 52: 2465 samples\n",
            "  Class 54: 519 samples\n",
            "  Total classes in train set: 32 / 58\n",
            "==================================================\n",
            "Epoch 1: Train: Loss: 0.6931, Micro_F1: 0.0838, Macro_F1: 0.0801, Layers: 10, Test: Loss: 0.6806, Micro_F1: 0.1650, Macro_F1: 0.1154, Layers: 10 - (1.1835s)\n",
            "------------ This was a new best! ----------\n",
            "Epoch 2: Train: Loss: 0.6801, Micro_F1: 0.2140, Macro_F1: 0.1446, Layers: 10, Test: Loss: 0.6579, Micro_F1: 0.0001, Macro_F1: 0.0003, Layers: 10 - (1.0647s)\n",
            "Epoch 3: Train: Loss: 0.6566, Micro_F1: 0.0005, Macro_F1: 0.0011, Layers: 10, Test: Loss: 0.6250, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0879s)\n",
            "Epoch 4: Train: Loss: 0.6224, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5815, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.1705s)\n",
            "Epoch 5: Train: Loss: 0.5773, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.5288, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.2094s)\n",
            "Epoch 6: Train: Loss: 0.5229, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4698, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.2197s)\n",
            "Epoch 7: Train: Loss: 0.4625, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.4084, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0682s)\n",
            "Epoch 8: Train: Loss: 0.4002, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.3494, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0517s)\n",
            "Epoch 9: Train: Loss: 0.3409, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10, Test: Loss: 0.2971, Micro_F1: 0.0000, Macro_F1: 0.0000, Layers: 10 - (1.0499s)\n",
            "Results for appnp: {'f1_micro': 0.16496895461132444, 'f1_macro': 0.1154189022976469, 'loss': 0.6805539131164551, 'total_time': 10.109044075012207, 'time_to_best': 1.183640956878662, 'total_iterations': 9, 'best_epoch': 1, 'time_per_iteration': 1.183640956878662}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "load_config = True\n",
        "dataset = 'Amazon'\n",
        "for portion in [0.05, 0.06, 0.07, 0.08, 0.09]:\n",
        "    params = {\n",
        "        'dataset': dataset,\n",
        "        'epochs': 10,\n",
        "        'self_loops': False,\n",
        "        'type': 'appnp',\n",
        "        'eps': 0.06,\n",
        "        'lr': 0.01,\n",
        "        'clip': 0.5,\n",
        "        'weight_decay': 1e-8,\n",
        "        'dropout': 0.5,\n",
        "        'width': 128,\n",
        "        'schedule': True,\n",
        "        'portion': portion\n",
        "    }\n",
        "    if load_config:\n",
        "        with open(appnp_config_path) as f:\n",
        "            loaded_config = json.load(f)\n",
        "        params = params | loaded_config\n",
        "\n",
        "    metrics = amazon_train_model(**params)\n",
        "\n",
        "    print(f'Results for {params[\"type\"]}: {metrics}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the data\n",
        "portions = [0.05, 0.06, 0.07, 0.08, 0.09]\n",
        "\n",
        "# Micro-F1 scores\n",
        "appnp_micro_f1 = [0.141830, 0.15095, 0.155393, 0.1640293, 0.16496]\n",
        "pprgnn_micro_f1 = [0.05274854825, 0.05274854, 0.053839987, 0.05429667704336, 0.0548360]\n",
        "\n",
        "# Macro-F1 scores\n",
        "appnp_macro_f1 = [0.109599, 0.11168, 0.112491, 0.11414, 0.115418]\n",
        "pprgnn_macro_f1 = [0.02446328, 0.02446328, 0.02484, 0.0249078343, 0.0250913]\n",
        "\n",
        "\n",
        "\n",
        "# Plotting function\n",
        "def plot_metric(y_appnp, y_pprgnn, metric_name):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(portions, y_appnp, label='APPNP', marker='o')\n",
        "    plt.plot(portions, y_pprgnn, label='PPRGNN', marker='s')\n",
        "    plt.title(f'{metric_name} vs Portion')\n",
        "    plt.xlabel('Portion')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate the plots\n",
        "plot_metric(appnp_micro_f1, pprgnn_micro_f1, 'Micro-F1')\n",
        "plot_metric(appnp_macro_f1, pprgnn_macro_f1, 'Macro-F1')\n",
        "\n"
      ],
      "metadata": {
        "id": "pGfbmim6ZiP8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "outputId": "aef906c1-1678-44de-e917-7263f689d61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSSapmgqLSjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}